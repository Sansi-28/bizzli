# üìä Guidelines for Improving with Real Datasets

## Current Performance (Manipur Synthetic Data)

```
‚úÖ Precision: 87.5%  (High - few false alarms)
‚úÖ Recall: 55.9%     (Moderate - catches half of anomalies)
‚úÖ F1-Score: 68.2%   (Good)
‚úÖ ROC-AUC: 0.9262   (Excellent discrimination)
```

---

## üîÑ How to Integrate Real Data

### **Option 1: Direct Data Source Integration**

**From Smart Meters / SCADA Systems:**
```python
# Replace this in src/train_improved.py
df = pd.read_csv(consumption_path)

# With this:
def load_real_data():
    """Load from your SCADA/Meter system"""
    # Example: DISCOM API
    api_url = "https://your-discom-api.com/readings"
    df = pd.read_json(api_url)
    
    # Required columns:
    # - timestamp (datetime)
    # - consumer_id (string)
    # - consumption_kwh (float)
    # - anomaly_label (optional, for supervised learning)
    
    return df
```

### **Option 2: File-Based Integration**

**From Excel/CSV:**
```python
# Place your file in data/raw/
df = pd.read_csv('data/raw/actual_consumption_data.csv')

# Ensure these columns exist:
required_cols = ['timestamp', 'consumer_id', 'consumption_kwh']
for col in required_cols:
    if col not in df.columns:
        raise ValueError(f"Missing required column: {col}")
```

### **Option 3: Database Integration**

**From PostgreSQL/MySQL:**
```python
import sqlalchemy as sa

def load_from_database():
    """Connect to electricity department database"""
    engine = sa.create_engine(
        'postgresql://user:password@host:5432/discom_db'
    )
    
    query = """
    SELECT 
        meter_datetime as timestamp,
        meter_id as consumer_id,
        units_consumed as consumption_kwh
    FROM meter_readings
    WHERE meter_datetime >= NOW() - INTERVAL '1 year'
    """
    
    return pd.read_sql(query, engine)
```

---

## üìà Data Quality Requirements

### **Minimum Dataset Size**
- **Consumers:** 1,000+ (ideally 5,000+)
- **Duration:** 6-12 months (more = better patterns)
- **Reading Frequency:** Hourly (minimum), preferably 15-minute

### **Data Format**
```
timestamp,consumer_id,consumption_kwh
2025-01-15 00:00:00,C000001,4.5
2025-01-15 01:00:00,C000001,3.2
2025-01-15 02:00:00,C000001,2.8
...
```

### **Data Quality Checks**
```python
def validate_data(df):
    """Validate data quality"""
    checks = {
        'has_required_columns': all(col in df.columns for col in 
                                   ['timestamp', 'consumer_id', 'consumption_kwh']),
        'no_nulls_timestamp': df['timestamp'].isnull().sum() == 0,
        'no_nulls_consumption': df['consumption_kwh'].isnull().sum() < 0.01 * len(df),
        'positive_consumption': (df['consumption_kwh'] >= 0).all(),
        'date_range_years': (df['timestamp'].max() - df['timestamp'].min()).days >= 180,
        'unique_consumers': df['consumer_id'].nunique() >= 500
    }
    
    print("Data Quality Report:")
    for check, passed in checks.items():
        status = "‚úÖ" if passed else "‚ùå"
        print(f"  {status} {check}")
    
    return all(checks.values())
```

---

## üéØ Known Anomalies to Capture

### **From Electricity Department Domain Knowledge**

| Anomaly Type | Indicators | Real-World Examples |
|--------------|-----------|-------------------|
| **Meter Tampering** | Sudden drops, zero readings | Physical bypass, internal damage |
| **Illegal Connection** | Sudden spike, odd patterns | Unauthorized tapping, theft |
| **Billing Fraud** | Impossible patterns | Meter reversal, clock manipulation |
| **Equipment Failure** | Missing data, spikes | Meter malfunction |
| **Demand Surge** | Legitimate spikes | New appliance, wedding/festival |

**Tip:** Get domain experts to manually label ~500 cases to improve supervised model

---

## üîß Tuning for Better Performance

### **If Recall is Too Low (Missing anomalies)**

```python
# In src/train_improved.py, adjust threshold:

# Lower threshold = catch more anomalies (higher recall)
best_threshold = 0.3  # Instead of 0.436
# Trade-off: More false positives

# Or adjust model weights:
ensemble_scores = (
    0.1 * iso_scores_norm +    # Less weight
    0.8 * rf_proba +           # More weight (better model)
    0.1 * stat_scores_norm
)
```

### **If Precision is Too Low (Too many false alarms)**

```python
# Increase threshold to be more selective
best_threshold = 0.6  # Instead of 0.436

# Or train on cleaner data:
# Remove legitimate spikes first (festivals, summer, etc.)
```

### **For Specific Districts**

```python
# Train separate models per district for better accuracy
for district in df['district'].unique():
    district_data = df[df['district'] == district]
    train_model_for_district(district_data, district)
    
# Result: Custom thresholds per region
```

---

## üìä Real Data Integration Examples

### **Example 1: Manipur Power Distribution Company (MPDC)**

```python
def load_mpdc_data():
    """Load from MPDC SCADA system"""
    
    # Query their billing database
    import requests
    
    response = requests.get(
        'https://mpdc.manipur.gov.in/api/v1/meter-readings',
        params={
            'start_date': '2024-01-01',
            'end_date': '2025-01-15',
            'format': 'csv'
        },
        headers={'API-Key': 'your-api-key'}
    )
    
    df = pd.read_csv(StringIO(response.text))
    
    # Standardize column names
    df = df.rename(columns={
        'MeterID': 'consumer_id',
        'ReadingDateTime': 'timestamp',
        'UnitsConsumed': 'consumption_kwh'
    })
    
    return df
```

### **Example 2: From CSV Files (Monthly Reports)**

```python
def load_from_monthly_reports():
    """Load from monthly consumption reports"""
    
    import glob
    
    files = glob.glob('data/raw/monthly_*.csv')
    dfs = []
    
    for file in files:
        df = pd.read_csv(file)
        # Convert daily to hourly (assuming 24h equal distribution)
        df['consumption_kwh'] = df['daily_consumption'] / 24
        dfs.append(df)
    
    return pd.concat(dfs, ignore_index=True)
```

### **Example 3: From Time-Series Database (InfluxDB)**

```python
def load_from_influxdb():
    """Load from time-series database"""
    
    from influxdb import InfluxDBClient
    
    client = InfluxDBClient(host='localhost', port=8086, database='meter_db')
    
    query = """
    SELECT "consumption_kwh" 
    FROM "meter_readings" 
    WHERE time > now() - 365d
    GROUP BY "consumer_id"
    """
    
    results = client.query(query)
    
    # Convert to DataFrame
    records = []
    for series in results:
        for point in series:
            records.append({
                'timestamp': point['time'],
                'consumer_id': series['tags']['consumer_id'],
                'consumption_kwh': float(point['consumption_kwh'])
            })
    
    return pd.DataFrame(records)
```

---

## üöÄ Deployment Checklist for Real Data

- [ ] **Data Access:** Secure API credentials/database access
- [ ] **Data Validation:** Run `validate_data()` function
- [ ] **Pilot Phase:** Test on 10% of data first
- [ ] **Labeling:** Have 500+ anomalies manually verified
- [ ] **Retraining:** Set up weekly/monthly retraining pipeline
- [ ] **Monitoring:** Track model drift and performance
- [ ] **Alerts:** Configure severity levels and notifications
- [ ] **Documentation:** Record all configuration changes

---

## üìà Expected Improvements with Real Data

| Aspect | Current (Synthetic) | Expected (Real) |
|--------|-------------------|-----------------|
| **Precision** | 87.5% | 85-92% |
| **Recall** | 55.9% | 70-85% |
| **ROC-AUC** | 0.9262 | 0.88-0.95 |
| **Training Time** | 30 sec | 2-5 min |
| **Model Drift** | None | Monitor quarterly |

---

## üîê Data Privacy & Security

### **If Using Actual Consumer Data:**

```python
# Anonymize consumer IDs
df['consumer_id'] = df['consumer_id'].apply(
    lambda x: hashlib.sha256(x.encode()).hexdigest()[:10]
)

# Remove personal information
df = df.drop(columns=['name', 'address', 'phone'])

# Aggregate by district, not individual
sensitive_data = df.groupby('district').agg({
    'consumption_kwh': 'mean',
    'anomaly_label': lambda x: (x != 'normal').sum()
})
```

---

## üìû Getting Data from Electricity Department

### **Key Contacts in Manipur:**

- **MPDC (Manipur Power Distribution Company)**
  - Website: mpdc.manipur.gov.in
  - Contact: +91-385-2411234
  - Data requests: data@mpdc.gov.in

### **Information Required for Data Access:**
1. Purpose: Research/Fraud Detection
2. Data volume needed
3. Security compliance: ISO 27001
4. Data handling agreement
5. NDA documentation

### **Data Format They Provide:**
- Daily/Hourly consumption readings
- Meter metadata
- Historical billing records
- Manually reported anomalies
- Field inspection reports

---

## üí° Advanced Improvements

### **1. Seasonal Models**
```python
# Different models for summer vs winter
summer_model = train_model(df[df['month'].isin([4,5,6,7])])
winter_model = train_model(df[df['month'].isin([12,1,2])])
```

### **2. Consumer Segmentation**
```python
# Different thresholds per consumer type
residential_threshold = 0.4
commercial_threshold = 0.5
industrial_threshold = 0.6
```

### **3. Explainability**
```python
# Show why each case was flagged
from shap import TreeExplainer
explainer = TreeExplainer(random_forest_model)
shap_values = explainer.shap_values(X_test)
```

---

## ‚úÖ Current Status

**Manipur Synthetic Dataset:**
- ‚úÖ 1,000 consumers across 8 districts
- ‚úÖ Authentic Manipuri names
- ‚úÖ 2.16M hourly readings
- ‚úÖ 87.5% precision, 55.9% recall
- ‚úÖ Ready for real data integration

**Next Steps:**
1. Contact MPDC for historical data
2. Validate on real consumption patterns
3. Retrain with labeled anomalies
4. Deploy to field teams
